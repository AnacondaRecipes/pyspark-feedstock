{% set name = "pyspark" %}
{% set version = "3.5.3" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  - url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
    sha256: 68b7cc0c0c570a7d8644f49f40d2da8709b01d30c9126cc8cf93b4f84f3d9747
  # get the LICENSE from the GH repo
  - url: https://raw.githubusercontent.com/apache/spark/refs/tags/v{{ version }}/LICENSE
    sha256: 80209caf5bfbc82ad9163d9a42d7be5cbdcbf26e4e2b13666cdc11e4729ab50d

build:
  number: 0
  # pyarrow isn't available on s390x.
  skip: True  # [py<38 or s390x]
  script: {{ PYTHON }} -m pip install . --no-deps --no-build-isolation --ignore-installed -vv

requirements:
  host:
    - pip
    - python
    - setuptools
    - wheel
  run:
    - python
    - py4j ==0.10.9.7
    # extras_require dependencies
    - numpy >=1.15,<2
    - pandas >=1.0.5
    - pyarrow >=4.0.0
    - grpcio >=1.56.0
    - grpcio-status >=1.56.0
    - googleapis-common-protos >=1.56.4

test:
  imports:
    - pyspark
    - pyspark.cloudpickle
    - pyspark.mllib
    - pyspark.ml.connect
    - pyspark.mllib.linalg
    - pyspark.mllib.stat
    - pyspark.ml
    - pyspark.ml.linalg
    - pyspark.ml.param
    - pyspark.ml.torch
    - pyspark.ml.deepspeed
    - pyspark.sql
    - pyspark.sql.avro
    - pyspark.sql.connect
    - pyspark.sql.connect.avro
    - pyspark.sql.connect.client
    - pyspark.sql.connect.proto
    - pyspark.sql.connect.protobuf
    - pyspark.sql.connect.streaming
    - pyspark.sql.pandas
    - pyspark.sql.protobuf
    - pyspark.sql.streaming
    - pyspark.streaming
    - pyspark.sql.connect.streaming.worker
    - pyspark.bin
    - pyspark.sbin
    - pyspark.jars
    - pyspark.pandas
    - pyspark.pandas.data_type_ops
    - pyspark.pandas.indexes
    - pyspark.pandas.missing
    - pyspark.pandas.plot
    - pyspark.pandas.spark
    - pyspark.pandas.typedef
    - pyspark.pandas.usage_logging
    - pyspark.python.pyspark
    - pyspark.python.lib
    - pyspark.testing
    - pyspark.data
    - pyspark.licenses
    - pyspark.resource
    - pyspark.errors
    - pyspark.errors.exceptions
    - pyspark.examples.src.main.python
  requires:
    - pip
  commands:
    - pip check
    - bash -c "compgen -c spark && compgen -c pyspark"  # [not win]
    - where *spark*                                     # [win]

about:
  home: https://spark.apache.org/
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE
  summary: Apache Spark Python API
  description: Apache Spark is a fast and general engine for large-scale data processing.
  dev_url: https://github.com/apache/spark/tree/master/python
  doc_url: https://spark.apache.org/documentation.html

extra:
  recipe-maintainers:
    - parente
    - ericdill
    - quasiben
    - dbast
    - mariusvniekerk
